{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6224110",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import glob\n",
    "import collections\n",
    "from joblib import Parallel, delayed\n",
    "import collections\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "import librosa, librosa.display\n",
    "import scipy\n",
    "from scipy.io import wavfile\n",
    "import soundfile as sf\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys \n",
    "import os\n",
    "import math\n",
    "import yaml\n",
    "import copy\n",
    "from scipy import signal\n",
    "import wave\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4173112b",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### helper functions ############\n",
    "\n",
    "## For data\n",
    "def pad(x, max_len=48000):\n",
    "    \"make sure all data samples have the same length\"\n",
    "    \n",
    "    x_len = x.shape[0]\n",
    "    if x_len >= max_len:\n",
    "        return x[:max_len]\n",
    "    # need to pad\n",
    "    num_repeats = int(max_len / x_len)+1\n",
    "    padded_x = np.tile(x, (1, num_repeats))[:, :max_len][0]  \n",
    "    return padded_x\n",
    "\n",
    "## For SincNet\n",
    "def next_power_of_2(x):  \n",
    "    return 1 if x == 0 else 2**(x - 1).bit_length()\n",
    "\n",
    "def to_2tuple(x):\n",
    "    if isinstance(x, collections.abc.Iterable):\n",
    "        return x\n",
    "    return (x, x)\n",
    "\n",
    "## For Transformer\n",
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "\n",
    "## For Optimizer\n",
    "def rate(step, model_size, factor, warmup):\n",
    "    if step == 0:\n",
    "        step = 1\n",
    "    return factor * (\n",
    "        model_size ** (-0.5) * min(step ** (-0.5), step * warmup ** (-1.5))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dca818e",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### helper class ############\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm,self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(features))\n",
    "        self.beta = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.gamma * (x - mean) / (std + self.eps) + self.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffc4e841",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SincConv(nn.Module):   \n",
    "    \"Apply SincNet filter to the raw audios by using Sinc-based convolution\"\n",
    "\n",
    "    @staticmethod\n",
    "    def to_mel(hz):\n",
    "        return 2595 * np.log10(1 + hz / 700)\n",
    "\n",
    "    @staticmethod\n",
    "    def to_hz(mel):\n",
    "        return 700 * (10 ** (mel / 2595) - 1)\n",
    "\n",
    "    def __init__(self, out_channels, kernel_size=128, sample_rate=16000, in_channels=1,\n",
    "                 stride=1, padding=0, dilation=1, bias=False, groups=1, min_low_hz=50, min_band_hz=50):\n",
    "\n",
    "        super(SincConv,self).__init__()\n",
    "\n",
    "        if in_channels != 1:\n",
    "            msg = \"SincConv only support one input channel (here, in_channels = {%i})\" % (in_channels)\n",
    "            raise ValueError(msg)\n",
    "\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        \n",
    "        if kernel_size%2==0:\n",
    "            self.kernel_size=self.kernel_size+1\n",
    "            \n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "\n",
    "        if bias:\n",
    "            raise ValueError('SincConv does not support bias.')\n",
    "        if groups > 1:\n",
    "            raise ValueError('SincConv does not support groups.')\n",
    "\n",
    "        self.sample_rate = sample_rate\n",
    "        self.min_low_hz = min_low_hz\n",
    "        self.min_band_hz = min_band_hz\n",
    "\n",
    "        low_hz = 30\n",
    "        high_hz = self.sample_rate / 2 - (self.min_low_hz + self.min_band_hz)\n",
    "\n",
    "        mel = np.linspace(self.to_mel(low_hz),\n",
    "                          self.to_mel(high_hz),\n",
    "                          self.out_channels + 1)\n",
    "        hz = self.to_hz(mel)\n",
    "        \n",
    "\n",
    "        self.low_hz_ = nn.Parameter(torch.Tensor(hz[:-1]).view(-1, 1))\n",
    "\n",
    "        self.band_hz_ = nn.Parameter(torch.Tensor(np.diff(hz)).view(-1, 1))\n",
    "\n",
    "        # Apply Hamming window\n",
    "        n_lin=torch.linspace(0, (self.kernel_size/2)-1, steps=int((self.kernel_size/2))) \n",
    "        self.window_=0.54-0.46*torch.cos(2*math.pi*n_lin/self.kernel_size);\n",
    "\n",
    "        n = (self.kernel_size - 1) / 2.0\n",
    "        self.band_pass = 2*math.pi*torch.arange(-n, 0).view(1, -1) / self.sample_rate \n",
    "\n",
    " \n",
    "\n",
    "    def forward(self, x):\n",
    "        self.band_pass = self.band_pass.to(x.device)\n",
    "\n",
    "        self.window_ = self.window_.to(x.device)\n",
    "\n",
    "        low = self.min_low_hz  + torch.abs(self.low_hz_)\n",
    "        \n",
    "        high = torch.clamp(low + self.min_band_hz + torch.abs(self.band_hz_),self.min_low_hz,self.sample_rate/2)\n",
    "        band=(high-low)[:,0]\n",
    "        \n",
    "        f_times_t_low = torch.matmul(low, self.band_pass)\n",
    "        f_times_t_high = torch.matmul(high, self.band_pass)\n",
    "\n",
    "        band_pass_left=((torch.sin(f_times_t_high)-torch.sin(f_times_t_low))/(self.band_pass/2))*self.window_ \n",
    "        band_pass_center = 2*band.view(-1,1)\n",
    "        band_pass_right= torch.flip(band_pass_left,dims=[1])\n",
    "        \n",
    "        \n",
    "        band_pass=torch.cat([band_pass_left,band_pass_center,band_pass_right],dim=1)\n",
    "\n",
    "        \n",
    "        band_pass = band_pass / (2*band[:,None])      \n",
    "\n",
    "        self.filters = (band_pass).view(self.out_channels, 1, self.kernel_size)\n",
    "\n",
    "        return F.conv1d(x, self.filters, stride=self.stride,\n",
    "                        padding=self.padding, dilation=self.dilation,\n",
    "                         bias=None, groups=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ae2720d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    \"Segment SincNet features into patches\"\n",
    "    \n",
    "    def __init__(self, feature_size, patch_size, embed_dim, in_chans=1):\n",
    "        super().__init__()\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        self.grid_size = (feature_size[0] // patch_size[0], feature_size[1] // patch_size[1])\n",
    "        self.num_patches = self.grid_size[0] * self.grid_size[1]\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, H, W = x.shape\n",
    "        x = x.view(B, 1, H, W)\n",
    "        x = self.proj(x)\n",
    "        x = x.flatten(2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4a681f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbedReduce(nn.Module):\n",
    "    \"Reduce the dimension of patches\"\n",
    "\n",
    "    def __init__(self, current_len, seq_size):    \n",
    "        super(EmbedReduce,self).__init__()\n",
    "        self.linear1=nn.Linear(current_len, seq_size[0])\n",
    "        self.lin_ln1 = nn.LayerNorm(seq_size[0])\n",
    "        self.linear2=nn.Linear(seq_size[0], seq_size[1])\n",
    "        self.lin_ln2 = nn.LayerNorm(seq_size[1])\n",
    "        self.linear3=nn.Linear(seq_size[1], seq_size[2])\n",
    "        self.lin_ln3 = nn.LayerNorm(seq_size[2])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.lin_ln1(x)\n",
    "        \n",
    "        x = self.linear2(x)\n",
    "        x = self.lin_ln2(x)\n",
    "        \n",
    "        x = self.linear3(x)\n",
    "        x = self.lin_ln3(x)\n",
    "\n",
    "        return x.transpose(1, 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f39c1a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PositionalEncoding function.\"\n",
    "\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)  \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, : x.size(1)].requires_grad_(False) \n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55850430",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### Class for Transformer ############\n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "    \n",
    "\n",
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = scores.softmax(dim=-1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "\n",
    "        query, key, value = [\n",
    "            lin(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "            for lin, x in zip(self.linears, (query, key, value))]\n",
    "\n",
    "        x, self.attn = attention(\n",
    "            query, key, value, mask=mask, dropout=self.dropout)\n",
    "\n",
    "        x = (x.transpose(1, 2).contiguous()\n",
    "            .view(nbatches, -1, self.h * self.d_k))\n",
    "        \n",
    "        del query\n",
    "        del key\n",
    "        del value\n",
    "        return self.linears[-1](x)\n",
    "    \n",
    "    \n",
    "class EncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, d_hidden, h, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadedAttention(h, d_model)\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model, d_hidden,)\n",
    "        self.sublayer = clones(SublayerConnection(d_model, dropout), 2)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"Follow Figure 1 (left) for connections.\"\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)\n",
    "    \n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, d_hidden, N, h, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(EncoderLayer(d_model, d_hidden, h, dropout), N)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask) \n",
    "        return x\n",
    "    \n",
    "    \n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_model, d_hidden, N=6, h=8,dropout=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_hidden = d_hidden\n",
    "        \n",
    "        self.encoder = Encoder(self.d_model, self.d_hidden, N, h, dropout)\n",
    "        \n",
    "        print('initialization: xavier')\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        src_mask = torch.cat((torch.zeros(x.shape[0],1,1),torch.ones(x.shape[0],1,x.shape[1]-1)),dim=2).cuda()\n",
    "        x = self.encoder(x,src_mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1c1926e",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### The entire model ############\n",
    "\n",
    "class Box(nn.Module):\n",
    "    def __init__(self, model_args, device):\n",
    "        super(Box, self).__init__()\n",
    "        \n",
    "        self.device = device\n",
    "\n",
    "        self.Sinc_conv = SincConv(out_channels = model_args['num_filter'],\n",
    "                                  kernel_size = model_args['filt_len'],\n",
    "                                  in_channels = model_args['in_channels'])\n",
    "        self.feature_dim = int((model_args['samp_len']-model_args['filt_len']+1)/model_args['max_pool_len'])\n",
    "        self.lnorm1 = LayerNorm([model_args['num_filter'],self.feature_dim])\n",
    "        self.leaky_relu = nn.LeakyReLU(0.2)\n",
    "        \n",
    "        self.d_embed = model_args['patch_embed']\n",
    "        self.patchEmbed = PatchEmbed(feature_size = (model_args['num_filter'], self.feature_dim),\n",
    "                                     patch_size = model_args['patch_size'],\n",
    "                                     embed_dim = self.d_embed)\n",
    "        self.seq_len = (model_args['num_filter']//model_args['patch_size'])*(self.feature_dim//model_args['patch_size'])\n",
    "        self.EmbedReduce = EmbedReduce(current_len = self.seq_len, seq_size = model_args['seq_size'])\n",
    "        \n",
    "        self.posEncode = PositionalEncoding(d_model = self.d_embed, dropout = model_args['drop_out'])\n",
    "        \n",
    "        self.transformer = Transformer(d_model = self.d_embed, \n",
    "                                       d_hidden = model_args['encoder_hidden'], \n",
    "                                       N = model_args['num_block'],\n",
    "                                       h = model_args['num_head'],\n",
    "                                       dropout = model_args['drop_out'])\n",
    "        \n",
    "        \n",
    "        self.mlp = nn.Sequential(nn.LayerNorm(model_args['seq_size'][2]),\n",
    "                                 nn.Linear(in_features = model_args['seq_size'][2], out_features = model_args['seq_size'][2]),\n",
    "                                 nn.Linear(in_features = model_args['seq_size'][2], out_features = model_args['nb_classes']))\n",
    "        \n",
    "       \n",
    "    def forward(self, x, y = None,is_test=False):\n",
    "        batch = x.shape[0]\n",
    "        len_seq = x.shape[1]\n",
    "        x = x.view(batch,1,len_seq)\n",
    "        \n",
    "        x = self.Sinc_conv(x)    \n",
    "        x = F.max_pool1d(torch.abs(x), 3)\n",
    "        x = self.lnorm1(x)\n",
    "        x = self.leaky_relu(x)\n",
    "        \n",
    "        x = self.patchEmbed(x)\n",
    "        x = self.EmbedReduce(x)\n",
    "\n",
    "        x = self.posEncode(x)\n",
    "        \n",
    "        x = self.transformer(x).transpose(1,2)\n",
    "        \n",
    "        x = self.mlp(x.mean(dim=1))\n",
    "        output=F.softmax(x,dim=1)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef44cea5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9ea906",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### Loading dataset ############\n",
    "AudioFile = collections.namedtuple('AudioFile',\n",
    "    ['file_name','path','label', 'key'])\n",
    "\n",
    "\n",
    "class ADDDataset(Dataset):\n",
    "    def __init__(self, data_path=None, label_path=None,transform=None,\n",
    "                 is_train=True,is_eval=False,feature=None,track=None):\n",
    "        self.data_path = data_path\n",
    "        self.label_path = label_path\n",
    "        self.transform = transform\n",
    "        self.track = track\n",
    "        self.feature = feature\n",
    "        \n",
    "        self.dset_name = 'eval' if is_eval else 'train' if is_train else 'dev'\n",
    "        cache_fname = 'cache_ADD_{}_{}.npy'.format(self.dset_name,self.track)\n",
    "        if (self.dset_name == 'eval'):\n",
    "            cache_fname = 'cache_ADD_{}_{}.npy'.format(self.dset_name,self.track)\n",
    "            self.cache_fname = os.path.join(\"/home/menglu/123/Deepfake/built\", cache_fname) #need to change the directory\n",
    "        else:   \n",
    "            cache_fname = 'cache_ADD_{}.npy'.format(self.dset_name)\n",
    "            self.cache_fname = os.path.join(\"/home/menglu/123/Deepfake/built\", cache_fname)\n",
    "  \n",
    "        if os.path.exists(self.cache_fname):\n",
    "            self.data_x, self.data_y, self.files_meta = torch.load(self.cache_fname)\n",
    "            print('Dataset loaded from cache', self.cache_fname)\n",
    "        else: \n",
    "            self.files_meta = self.parse_protocols_file(self.label_path)\n",
    "            data = list(map(self.read_file, self.files_meta))\n",
    "            self.data_x, self.data_y= map(list, zip(*data))\n",
    "            if self.transform:\n",
    "                self.data_x = Parallel(n_jobs=5, prefer='threads')(delayed(self.transform)(x) for x in self.data_x)                          \n",
    "            torch.save((self.data_x, self.data_y, self.files_meta), self.cache_fname)\n",
    "        \n",
    "    def __len__(self):\n",
    "        self.length = len(self.data_x)\n",
    "        return self.length\n",
    "   \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data_x[idx]\n",
    "        y = self.data_y[idx]\n",
    "        return x, y\n",
    "    \n",
    "    def read_file(self, meta):   \n",
    "        data_x, sample_rate = librosa.load(meta.path,sr=16000)       \n",
    "        data_y = meta.key\n",
    "        return data_x, float(data_y)\n",
    "      \n",
    "    def parse_line(self,line):\n",
    "        tokens = line.strip().split(' ')\n",
    "        audio_path=os.path.join(self.data_path, tokens[0]).replace('\\\\','/')\n",
    "        return AudioFile(file_name=tokens[0], path = audio_path,\n",
    "                         label=tokens[1], key=int(tokens[1] == 'genuine'))\n",
    "        \n",
    "    def parse_protocols_file(self, label_path):\n",
    "        lines = open(label_path).readlines()\n",
    "        files_meta = map(self.parse_line, lines)\n",
    "        return list(files_meta)\n",
    "\n",
    "    def __len__(self):\n",
    "        self.length = len(self.data_x)\n",
    "        return self.length\n",
    "   \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data_x[idx]\n",
    "        y = self.data_y[idx]\n",
    "        return x, y\n",
    "    \n",
    "    def read_file(self, meta):   \n",
    "        data_x, sample_rate = librosa.load(meta.path,sr=16000)       \n",
    "        data_y = meta.key\n",
    "        return data_x, float(data_y)\n",
    "      \n",
    "    def parse_line(self,line):\n",
    "        tokens = line.strip().split(' ')\n",
    "        audio_path=os.path.join(self.data_path, tokens[0]).replace('\\\\','/')\n",
    "        return AudioFile(file_name=tokens[0], path = audio_path,\n",
    "                         label=tokens[1], key=int(tokens[1] == 'genuine'))\n",
    "        \n",
    "    def parse_protocols_file(self, label_path):\n",
    "        lines = open(label_path).readlines()\n",
    "        files_meta = map(self.parse_line, lines)\n",
    "        return list(files_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7815092b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader for TRAINING set\n",
    "database_path = \"/home/menglu/123/Dataset/ADD2022/ADD_train_dev/train\"  #path of folder that stores traning data\n",
    "label_path = \"/home/menglu/123/Dataset/ADD2022/label/train_label.txt\"\n",
    "transform = transforms.Compose([\n",
    "    lambda x: pad(x),\n",
    "    lambda x: Tensor(x)\n",
    "])\n",
    "batch_size = 32\n",
    "\n",
    "train_set = ADDDataset(data_path=database_path,label_path=label_path,is_train=True, transform=transform)\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True,drop_last=True)\n",
    "\n",
    "# Dataloader for VALIDATION set\n",
    "dev_data_path = \"/home/menglu/123/Dataset/ADD2022/ADD_train_dev/dev\"\n",
    "dev_label_path = \"/home/menglu/123/Dataset/ADD2022/label/dev_label.txt\"\n",
    "\n",
    "dev_set = ADDDataset(data_path = dev_data_path,label_path = dev_label_path,is_train=False, transform=transform)\n",
    "dev_loader = DataLoader(dev_set, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897623c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189c8af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(data_loader, model, lr, optim, device, scheduler = None):\n",
    "    \"Function for training process\"\n",
    "    \n",
    "    running_loss = 0\n",
    "    num_correct = 0.0\n",
    "    num_total = 0.0\n",
    "    model.train()\n",
    "    weight = torch.FloatTensor([1.0, 9.0]).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=weight)\n",
    "\n",
    "    for batch_x, batch_y in data_loader:   \n",
    "        batch_size = batch_x.size(0)\n",
    "        num_total += batch_size\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.view(-1).type(torch.int64).to(device)\n",
    "        batch_out = model(batch_x,batch_y)\n",
    "        batch_loss = criterion(batch_out, batch_y)\n",
    "        _, batch_pred = batch_out.max(dim=1)\n",
    "        num_correct += (batch_pred == batch_y).sum(dim=0).item()\n",
    "        running_loss += (batch_loss.item() * batch_size)\n",
    "\n",
    "        optim.zero_grad()\n",
    "        batch_loss.backw'+current_model+'ard()\n",
    "        optim.step()\n",
    "        if scheduler !=None:\n",
    "            scheduler.step()\n",
    "       \n",
    "    running_loss /= num_total\n",
    "    train_accuracy = (num_correct/num_total)*100\n",
    "    return running_loss, train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1b6274",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_loader, model, device):\n",
    "    \"Function for validation process\"\n",
    "    \n",
    "    num_correct = 0.0\n",
    "    num_total = 0.0\n",
    "    model.eval()\n",
    "\n",
    "    for batch_x, batch_y in data_loader:\n",
    "        batch_size = batch_x.size(0)\n",
    "        num_total += batch_size\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.view(-1).type(torch.int64).to(device)\n",
    "        batch_out = model(batch_x,batch_y)\n",
    "        _, batch_pred = batch_out.max(dim=1)\n",
    "        num_correct += (batch_pred == batch_y).sum(dim=0).item()\n",
    "    return 100 * (num_correct / num_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df83cc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "# GPU device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Parameter\n",
    "config = yaml.safe_load(open('model_config.yaml'))\n",
    "lr = config['lr']\n",
    "warmup = config['warmup']\n",
    "num_epochs = config['epoch']\n",
    "\n",
    "d_model = config['model']['patch_embed']\n",
    "num_filter = config['model']['num_filter']\n",
    "num_block = config['model']['num_block']\n",
    "num_head = config['model']['num_head']\n",
    "\n",
    "# Model Initialization\n",
    "model = Box(config['model'],device).to(device)\n",
    "nb_params = sum([param.view(-1).size()[0] for param in model.parameters()])\n",
    "print(nb_params)\n",
    "\n",
    "# Adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), \n",
    "                             lr = lr, betas=(0.9, 0.98), eps=1e-9)\n",
    "lr_scheduler = LambdaLR(optimizer=optimizer,\n",
    "                        lr_lambda=lambda step: rate(step, d_model, factor=1, warmup=config[\"warmup\"]),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eee6c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "### create folder to save model parameters\n",
    "model_tag = 'SincNet_Transformer_{}_{}_{}_{}_{}_{}'.format(batch_size,d_model, num_filter, num_block, num_head,lr)\n",
    "\n",
    "#need to change the directory\n",
    "model_save_path = os.path.join('/home/menglu/123/Deepfake/built', model_tag)\n",
    "os.makedirs(model_save_path)\n",
    "print(model_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685d3b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and validation \n",
    "writer = SummaryWriter('logs/{}'.format(model_tag))\n",
    "best_acc = 0\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss, train_accuracy = train_epoch(train_loader,model, lr,optimizer, device, lr_scheduler)\n",
    "    valid_accuracy = evaluate_accuracy(dev_loader, model, device)\n",
    "    writer.add_scalar('train_accuracy', train_accuracy, epoch)\n",
    "    writer.add_scalar('valid_accuracy', valid_accuracy, epoch)\n",
    "    writer.add_scalar('loss', running_loss, epoch)\n",
    "    print('\\n{} - {} - {:.4f} - {:.4f}'.format(epoch,\n",
    "                                               running_loss, train_accuracy, valid_accuracy))\n",
    "    best_acc = max(valid_accuracy, best_acc)\n",
    "    torch.save(model.state_dict(), os.path.join(model_save_path, 'epoch_{}.pth'.format(epoch)))\n",
    "\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
